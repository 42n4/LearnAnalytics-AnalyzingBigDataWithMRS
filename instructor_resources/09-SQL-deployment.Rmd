---
title: "SQL Server deployment"
author: "Seth Mottaghinejad"
output: github_document
date: "`r Sys.Date()`"
---

```{r chap09chunk01, include=FALSE}
source('setup.R')
```


A basic overview of the SQL Server R Services architecture can be found [here](https://msdn.microsoft.com/en-us/library/mt604885.aspx). Let's point to a SQL table containing a copy of the NYC Taxi dataset. The first thing we need to do is set up a SQL Server _connection string_, which contains our SQL login credentials. Since the connection string contains sensitive information, it is usually stored in a file in a restricted location and read from R, but in our example we will simply hard-code the connection string and store it in `sqlConnString`. Assume, the NYC Taxi dataset is stored in a table called `NYCTaxiSmall` inside the `RDB` database that the connection string points to. The last thing left for us to do is to point to the table, which we do with the `RxSqlServerData` function. This is the equivalent of `RxXdfData` when pointing to an XDF file stored on disk.

To run the examples in this chapter, we need SQL Server 2016 with R Services installed (the stand-alone R Server is not needed). Instructions are shown [here](https://msdn.microsoft.com/en-us/library/mt696069.aspx). Once R Services is installed we need to enable it by running the following script:

```{sql, eval=FALSE}
-- let's enable external scripts so that SQL Server can make calls to the R server
EXEC sp_configure  'external scripts enabled', 1  
Reconfigure  with  override  
```

At this point we need to restart SQL Server, after which we can run the following script to make sure that `sp_configure` is set to 1.

```{sql, eval=FALSE}
-- you need to restart SQL Server at this point then run this to double check
EXEC sp_configure  'external scripts enabled'
```

Our next challenge is to create a new user with the proper permissions to execute R jobs. We also create a new database called `RDB` which we will use to run all the examples in this chapter.

```{sql, eval=FALSE}
USE master; 
GO  
CREATE DATABASE RDB;
GO

USE [master]
GO
CREATE LOGIN [ruser] WITH PASSWORD=N'ruser', DEFAULT_DATABASE=[master], CHECK_EXPIRATION=OFF, CHECK_POLICY=OFF
GO
USE [RDB]
GO
CREATE USER [ruser] FOR LOGIN [ruser]
ALTER ROLE [db_datareader] ADD MEMBER [ruser]
ALTER ROLE [db_datawriter] ADD MEMBER [ruser]
ALTER ROLE [db_ddladmin] ADD MEMBER [ruser]
GO

USE RDB
GO  
GRANT EXECUTE ANY EXTERNAL SCRIPT  TO [ruser] 
GO

use RDB
GO
GRANT EXECUTE TO [ruser]
GO
```

[This article](https://msdn.microsoft.com/en-us/library/mt604885.aspx) covers in more detail the two architectures involved when interacting with R Services. In either scenario we assume that we have SQL Server with R Services installed (usually on a remote VM to which we have limited access). This means there's an instance of Microsoft R Server installed on the same VM as the one hosting SQL Server. By default, it will go into "C:\Program Files\Microsoft SQL Server\MSSQL13.[SERVERNAME]\R_SERVICES\" where `SERVERNAME` is the name of the SQL Server instance. This R instance will be invoked by SQL Server to run R jobs. Data will go from SQL to this R instance, and sometimes back to SQL. We do not directly interact with this R instance, we only do so through SQL. There are two ways to do this:

  - The first scenario is better-suited for development. In this case, the data scientist has an interactive R session open on a client machine (such as a laptop). This R session will occasionally point to data in SQL Server and run some R code on it. For this to happen in-database, the R code needs to run not on the client machine, but on the remote SQL Server machine. By setting the compute context to SQL Server, the data scientist can remotely execute their R code. Any results generated by the R code is then sent back from the remote R session to the client R session for inspection.

![Development architecture](https://i-msdn.sec.s-msft.com/dynimg/IC854862.jpeg)

  - The second scenario is better-suited when the R code is tested and mature and needs to be used in production. In this case we simply wrap our R code inside of a stored procedure. This stored procedure can be invoked by any application or SQL user with the right permissions. When invoked, R Services is launched, data is usually passed to it from SQL Server, and the R code is executed.

![Deployment architecture](https://i-msdn.sec.s-msft.com/dynimg/IC851080.jpeg)

Here is a very basic example of a stored procedure invoking R Services. We can run this to make sure that R Services is properly installed and working for us. In this case, a SQL table is created on the fly by running `select 1 as hello`. This table is then passed to R, where it turns into a `data.frame` that by default is called `InputDataSet`. R then runs the code `OutputDataSet <- InputDataSet`, which copies this `data.frame` into a new one called `OutputDataSet`. Calling the new `data.frame` in R `OutputDataSet` will by default send it from R to SQL where it is not a SQL table. In this case we use `with result set` to set the schema for the table on the fly and display it.

```{sql, eval=FALSE}
-- here's a very basic example we can run to make sure everything worked
EXEC sp_execute_external_script  @language =N'R',  
@script=N'OutputDataSet <- InputDataSet',    
@input_data_1 =N'select 1 as hello'  
with result sets (([hello] int not null));  
GO
```

If the above examples fail to run, R Services is either not installed or not properly configured. The following are some common reasons for it: 

  - SQL Server authentication is not enabled
  - The launchpad service is not running
  - We did not restart the server

Here's an example shows the path where R Services is installed and where libraries are R installed. When we need to install a new R library in R Services, a SQL Server admin would log into the SQL Server machine and launch R as administrator and install the necessary packages.

```{sql, eval=FALSE}
-- this could be useful for debugging purposes
EXEC sp_execute_external_script  @language =N'R',  
@script=N'print(.libPaths())
          print(R.home())'
GO
```

Assuming that so far the examples ran successfully, we are now ready run SQR Server R Services. We will cover examples of both the development scenario and the deployment scenario that we covered above, starting with the development scenario. To begin with, we take the raw data into SQL:

```{sql, eval=FALSE}
USE RDB
GO

DROP TABLE NYCTaxiSmall;

CREATE TABLE NYCTaxiSmall(
  pickup_datetime       char(19),
  dropoff_datetime      char(19),
  passenger_count       tinyint,
  trip_distance         float(24),
  pickup_longitude      float(24),
  pickup_latitude       float(24),
  rate_code_id          char(1),
  dropoff_longitude     float(24),
  dropoff_latitude      float(24),
  payment_type          char(1),
  fare_amount           float(24),
  extra                 float(24),
  mta_tax               float(24),
  tip_amount            float(24),
  tolls_amount          float(24),
  improvement_surcharge float(24),
  total_amount          float(24))
GO

USE RDB
GO

DECLARE @cnt INT = 1;
WHILE @cnt < 7
BEGIN
    DECLARE @CSVfile nvarchar(255);
    SET @CSVfile = N'C:\Data\NYC_taxi\yellow_tripsample_2016-0'+cast(@cnt as char(1))+'.csv';
    PRINT @CSVfile;
    DECLARE @q nvarchar(MAX);
    SET @q=
       'BULK INSERT NYCTaxiSmall
        FROM '+char(39)+@CSVfile+char(39)+'
        WITH (FIRSTROW = 2, FIELDTERMINATOR = '','', ROWTERMINATOR = ''\n'')'
        -- WITH (FIRSTROW = 2, LASTROW = 1000000, FIELDTERMINATOR = '','', ROWTERMINATOR = ''\n'')'
    EXEC(@q)
    SET @cnt = @cnt + 1;
END;
GO
```

The content of the original 6 CSV files were now placed inside a SQL table called `NYCTaxiSmall`. We can now point to this table in R:

```{r chap09chunk02}
sqlConnString <- "Driver=SQL Server;Server=.;Database=RDB;Uid=ruser;Pwd=ruser"
sqlRowsPerRead <- 100000
sqlTable <- "NYCTaxiSmall"

nyc_raw <- RxSqlServerData(connectionString = sqlConnString,
                           rowsPerRead = sqlRowsPerRead, 
                           table = sqlTable)

rxGetInfo(nyc_raw, getVarInfo = TRUE, numRows = 5)
```

That's it. We can now use `nyc_raw` the same way we used `nyc_xdf` before. There is however something missing: we did not specify what the column types were. In this case, `RxSqlServerData` will try as best it can to convert a SQL Server column type to an R column type. This can cause problems though. First of all, SQL Server has a richer variety of column types than R. Second, some SQL Server column types like `datetime` for example don't always successfully transfer to their corresponding R column type. Third, the R column type `factor` does not really have a good equivalent in SQL Server, so in order for a column to be brought in as `factor` we must manually specify it. Doing so however gives us the advantage that we can also specify the levels and labels for it, and as we saw they don't always have to be the exact levels we see in the data. For example, if `payment_type` is represented by the integers 1 through 5 in the data, but we only care about 1 and 2 and want them labeled `card` and `cash` respectively, we can do that here without needing to do it later as a separate transformation. To deal with column types we create an object that stores the information about the columns and pass it to the `colInfo` argument in `RxSqlServerData`. Here's the example for `nyc_raw`:

```{r chap09chunk03}
rate_levels <- c("standard", "JFK", "Newark", "Nassau or Westchester", "negotiated", "group ride")

ccColInfo <- list(
pickup_datetime    = list(type = "character"),
dropoff_datetime   = list(type = "character"),
passenger_count    = list(type = "integer"),
rate_code_id       = list(type = "factor", levels = as.character(1:6), newLevels = rate_levels),
store_and_fwd_flag = list(type = "factor", levels = c("Y", "N")),
payment_type       = list(type = "factor", levels = as.character(1:2), newLevels = c("card", "cash"))
)

nyc_raw <- RxSqlServerData(connectionString = sqlConnString,
                           rowsPerRead = sqlRowsPerRead, 
                           table = sqlTable,
                           colInfo = ccColInfo)
```

At this point, the rest of the analysis is no different from what it was with the XDF file, so we can change `nyc_xdf` into `nyc_sql` and run the remaining code just like before. For example, we can start with `rxGetInfo` to double check the column types.

```{r chap09chunk04}
rxGetInfo(nyc_raw, getVarInfo = TRUE, numRows = 5)
```

Notice that the object above does not necessarily have to specify the types for each column in the data. We can limit it only to the columns of interest, and even then only the ones that need to be explicitly overwritten. However, since we tend to be more conservative in a production environment, it's best to be more explicit. After all, certain numeric columns in SQL Server could be stored as something different \(`VARCHAR` for example\) which would turn into a `character` column in R.

Let's now run `rxSummary` on a column of the data:

```{r chap09chunk05}
system.time(
  rxsum_sql <- rxSummary( ~ fare_amount, nyc_raw, reportProgress = 0)
)
```

We get our summary back, but something important is missing. We have not yet set the compute context to the remote SQL Server session. Although we got our summary back, because the compute context was set to the local R session \(the default\) `rxSummary` had to download the data \(using an ODBC connection\) to the local R session so that it could summarize it. **In-database analytics** however is about taking the data to the computation, not the other way around. So let's now set the compute context to the remote SQL Server session \(using `rxSetComputeContext`\).

```{r chap09chunk06}
# Set ComputeContext. Needs a temp directory path to serialize R objects back and forth
sqlShareDir <- paste("C:/AllShare/", Sys.getenv("USERNAME"), sep = "")
sqlWait <- TRUE
sqlConsoleOutput <- FALSE
sqlCC <- RxInSqlServer(connectionString = sqlConnString,
                       shareDir = sqlShareDir, 
                       wait = sqlWait, 
                       consoleOutput = sqlConsoleOutput)
```

And with the compute context now set to SQL Server we now rerun `rxSummary`.

```{r chap09chunk07}
rxSetComputeContext(sqlCC)
system.time(
  rxsum_sql <- rxSummary( ~ fare_amount, nyc_raw)
)
```

We can set the compute context back to local anytime we need to by running `rxSetComputeContext(RxLocalSeq())`. We can also run `rxGetComputeContext()` to see what the current compute context is. The difference in run time between `rxSummary` when the compute context is set to local and when it is set to SQL depends mostly on the size of the data and the speed at which it can travel over the network to reach the local R session \(when the compute context is set to local\). For large enough data sizes, this difference can be dramatic. By avoiding this cost, in-database analytics means we can greatly reduce our runtime.

There is one important distinction between working with data in a local compute context and working with data in SQL: when running a data transformation using `rxDataStep` we cannot overwrite a SQL table to add a new column. So we must write results to a new table and the schema for the new table must be declared ahead of time. This has two implications: 

 - 1. We should take advantage of on-the-fly transformations as much as possible when that's the right thing to do so that we can get faster results.
 - 2. If everytime we do a transformation we need to write it out to a new location, then we should combine our transformations when possible (we can simply place them inside the same transformation function) to avoid too many instances of the data. We can also manually remove prior instances if they're no longer needed. This brings out an important point: although for the most part our code is the same going from a local compute context to a remote one, when it comes to running common data-processing tasks (prior to running data summaries or modeling), we would benefit from taking a second pass and streamlining the data-processing steps as much as possible.

Now let's see how we can combine all the transformations we performed on the data in order to prepare it for analysis. We will repeate the last transformation, but this time combine it with all the other transformations we did to get the data ready for analysis. The first set of transformations we add involve creating columns `pickup_hour`, `pickup_dow`, `dropoff_hour`, `dropoff_dow`, and `trip_duration`. The second set of transformations use the Zillow neighborhoods shapefile to extract `pickup_nhood` and `dropoff_nhood`. Because the shapefile included neighborhoods outside of Manhattan (our area of interest), we also preform transformations that removed these unwanted neighborhoods from the levels of these `factor` columns. The details of the transformations are not discussed here as they were already discussed in Chapter 3. We will combine all of the above transformations into one transformation, thereby reducing some of the IO innefficiency of doing them piece by piece. As we can see, the resulting transformation function is a rather long one, but it not complex becuase it is just a combination of multiple transformations which we already developed and tested before.

```{r chap09chunk08}
xforms <- function(data) {

  data$tip_percent <- ifelse(data$fare_amount > 0, data$tip_amount/data$fare_amount, NA)

  weekday_labels <- c('Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')
  cut_levels <- c(1, 5, 9, 12, 16, 18, 22)
  hour_labels <- c('1AM-5AM', '5AM-9AM', '9AM-12PM', '12PM-4PM', '4PM-6PM', '6PM-10PM', '10PM-1AM')

  # extract pick-up hour and day of week
  pickup_datetime <- lubridate::ymd_hms(data$pickup_datetime, tz = "UTC")
  pickup_hour <- addNA(cut(hour(pickup_datetime), cut_levels))
  pickup_dow <- factor(wday(pickup_datetime), levels = 1:7, labels = weekday_labels)
  levels(pickup_hour) <- hour_labels
  # extract drop-off hour and day of week
  dropoff_datetime <- lubridate::ymd_hms(data$dropoff_datetime, tz = "UTC")
  dropoff_hour <- addNA(cut(hour(dropoff_datetime), cut_levels))
  dropoff_dow <- factor(wday(dropoff_datetime), levels = 1:7, labels = weekday_labels)
  levels(dropoff_hour) <- hour_labels
  data$pickup_hour <- pickup_hour
  data$pickup_dow <- pickup_dow
  data$dropoff_hour <- dropoff_hour
  data$dropoff_dow <- dropoff_dow
  # extract trip duration
  data$trip_duration <- as.integer(lubridate::interval(pickup_datetime, dropoff_datetime))
  
  # extract pick-up lat and long and find their neighborhoods
  pickup_longitude <- ifelse(is.na(data$pickup_longitude), 0, data$pickup_longitude)
  pickup_latitude <- ifelse(is.na(data$pickup_latitude), 0, data$pickup_latitude)
  data_coords <- data.frame(long = pickup_longitude, lat = pickup_latitude)
  coordinates(data_coords) <- c('long', 'lat')
  nhoods <- over(data_coords, shapefile)
  # add only the pick-up neighborhood and borough columns to the data
  data$pickup_nhood <- nhoods$Name
  data$pickup_borough <- nhoods$County

  # extract drop-off lat and long and find their neighborhoods
  dropoff_longitude <- ifelse(is.na(data$dropoff_longitude), 0, data$dropoff_longitude)
  dropoff_latitude <- ifelse(is.na(data$dropoff_latitude), 0, data$dropoff_latitude)
  data_coords <- data.frame(long = dropoff_longitude, lat = dropoff_latitude)
  coordinates(data_coords) <- c('long', 'lat')
  nhoods <- over(data_coords, shapefile)
  # add only the drop-off neighborhood and borough columns to the data
  data$dropoff_nhood <- nhoods$Name
  data$dropoff_borough <- nhoods$County

  # reduce pick-up and drop-off neighborhoods to manhattan only
  data$pickup_nb = factor(data$pickup_nhood, levels = nhoods_levels)
  data$dropoff_nb = factor(data$dropoff_nhood, levels = nhoods_levels)

  return(data)
}
```

It is time to run the above transformation function. We first make sure that any R objects that the function depends on is loaded so that we can pass it to the transformation function when we run `rxDataStep`.

```{r chap09chunk09}
library(rgeos)
library(maptools)

nyc_shapefile <- readShapePoly('../ZillowNeighborhoods-NY/ZillowNeighborhoods-NY.shp')
mht_shapefile <- subset(nyc_shapefile, City == 'New York' & County == 'New York')

manhattan_nhoods <- subset(nyc_shapefile@data, County == 'New York', select = "Name", drop = TRUE)
manhattan_nhoods <- as.character(manhattan_nhoods)
bad_nhoods <- c('Brooklyn Heights', 'Marble Hill', 'Mill Rock Park','Vinegar Hill')
bad_nhoods <- c(bad_nhoods, grep('Island', manhattan_nhoods, value = TRUE))
manhattan_nhoods <- setdiff(manhattan_nhoods, bad_nhoods)
```

We also create the new table that we will write the results to and declare its column types in SQL. The new table will be called `NYCTaxiSmallFeaturized`. Its colums are the original columns from `NYCTaxiSmall` and any columns that the above transformation creates.

```{sql, eval=FALSE}
CREATE TABLE NYCTaxiSmallFeaturized(
  pickup_datetime       char(19),
  dropoff_datetime      char(19),
  passenger_count       tinyint,
  trip_distance         float(24),
  pickup_longitude      float(24),
  pickup_latitude       float(24),
  rate_code_id          varchar(24),
  dropoff_longitude     float(24),
  dropoff_latitude      float(24),
  payment_type          char(5),
  fare_amount           float(24),
  extra                 float(24),
  mta_tax               float(24),
  tip_amount            float(24),
  tolls_amount          float(24),
  improvement_surcharge float(24),
  total_amount          float(24),
  tip_percent           float(24),
  pickup_hour           char(8),
  pickup_dow            char(3),
  dropoff_hour          char(8),
  dropoff_dow           char(3),
  trip_duration         int,
  pickup_nhood          varchar(30),
  pickup_borough        varchar(30),
  dropoff_nhood         varchar(30),
  dropoff_borough       varchar(30),
  pickup_nb             varchar(30),
  dropoff_nb            varchar(30))
GO
```

Notice that we've changed the column type for `rate_code_id` and `payment_type`. This is because in `nyc_raw` those columns were specified as `factor` and when a `factor` column is stored in SQL Server it becomes a `character` column and we need to make it long enough to store its levels as strings. We can now point to the above table in R, just as we did before and run `rxDataStep` to perform the transformation. However, since the transformation is happening in-database, we need to ensure that the packages needed to run the transformation have been installed on the R Server that SQL will invoke.

```{r chap09chunk10}
nyc_sql <- RxSqlServerData(connectionString = sqlConnString,
                           rowsPerRead = sqlRowsPerRead, 
                           table = "NYCTaxiSmallFeaturized")

rxDataStep(nyc_raw, nyc_sql, transformFunc = xforms, overwrite = TRUE, 
           transformPackages = c("lubridate", "sp", "maptools"),
           transformObjects = list(nhoods_levels = manhattan_nhoods,
                                   shapefile = nyc_shapefile))
```

Lastly, we need to update the column types in R so that the new columns we created through the above transformation can have the right format in R. Once again, it is not necessary to do this for all columns in the data, just the ones we intend to use for the analysis.

```{r chap09chunk11}
weekday_labels <- c('Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')
hour_labels <- c('1AM-5AM', '5AM-9AM', '9AM-12PM', '12PM-4PM', '4PM-6PM', '6PM-10PM', '10PM-1AM')

ccColInfo$pickup_dow <- list(type = "factor", levels = weekday_labels)
ccColInfo$pickup_hour <- list(type = "factor", levels = hour_labels)
ccColInfo$dropoff_dow <- list(type = "factor", levels = weekday_labels)
ccColInfo$dropoff_hour <- list(type = "factor", levels = hour_labels)
```

When working with the XDF file in the previous weeks, we went back and forth quite a few times to get the data in the right format, especially where `factor` columns were concerned. This is because we were learning about the data as we went and gradually preparing it for analysis. When working in a production environment however, the assumption is that we have our EDA behind us and know quite a bit about the data already. If not, then a recommended approach would be to take a sample of the data first and run some EDA on it. So some of the steps that we took with the XDF file in the prior weeks may have contained some redundancy or inneficiencies, which we never bothered to go back and revise. But when deploying code in production it's a good idea to make a second pass at the code and simplify things wherever it's warranted. As an example, when working with the XDF file, we first wrote a function to extract the `pickup_nb` and `dropoff_nb` columns from the pick-up and drop-off coordinates. We then noticed that those columns contain neighborhoods outside of Manhattan limits \(our area of interest\), so we made a second pass through the data to remove the factor levels for the irrelevant neighborhoods. With `nyc_sql`, we could take a similar approach: read those columns as `factors` with levels as is, and then use `rxDataStep` to perform a transformation that removes unwanted factor levels. But doing so is inefficient. The better approach is to find all the relevant factor levels \(Manhattan neighborhoods, which we can get directly from the shapefile\) and in the `ccColInfo` object only specify those as levels for those columns. Here's how:

```{r chap09chunk12}
library(rgeos)
library(maptools)

nyc_shapefile <- readShapePoly('../ZillowNeighborhoods-NY/ZillowNeighborhoods-NY.shp')
mht_shapefile <- subset(nyc_shapefile, City == 'New York' & County == 'New York')

manhattan_nhoods <- subset(nyc_shapefile@data, County == 'New York', select = "Name", drop = TRUE)
manhattan_nhoods <- as.character(manhattan_nhoods)
bad_nhoods <- c('Brooklyn Heights', 'Marble Hill', 'Ellis Island', 'Liberty Island', 'Mill Rock Park', 'Governors Island', 'Vinegar Hill')
manhattan_nhoods <- setdiff(manhattan_nhoods, bad_nhoods)

ccColInfo$pickup_nb <- list(type = "factor", levels = manhattan_nhoods)
ccColInfo$dropoff_nb <- list(type = "factor", levels = manhattan_nhoods)
```

And finally, we point to the SQL table a second time, but this time specify how columns should be treated in R using the `colInfo` argument.

```{r chap09chunk13}
nyc_sql@colInfo <- ccColInfo
rxGetInfo(nyc_sql, getVarInfo = TRUE, numRows = 5)
```

Just recall that every time we make a change to `ccColInfo`, we need to rerun the above line so that the change is reflected. For example, later \(after running the `seriate` function\), we can reorder the factor levels so that instead of being alphabetically ordered as they are now, they can follow a more natural ordering based on proximity to each other.

There are however some limitations with a SQL compute context that we need to be aware of. Certain functions, such as `rxMerge`, `rxSort` or `rxSplit` only work with XDF files on the local file system, not with data sitting in Spark or SQL Server. This is because the common data processing functions already have their \(probably more efficient\) implementation, so we can simply defer to the SQL language if we need to join tables, sort tables, or split tables instead of the above-mentioned `RevoScaleR` functions.

As an example, let's run the same linear model we build on the XDF file now using the SQL table. We're going to build the model on 75 percent of the data \(the training data\) by creating a column `u` of random uniform numbers and using `rowSelection` only picking rows where `u < .75`.

```{r chap09chunk14}
system.time(linmod <- rxLinMod(tip_percent ~ pickup_nb:dropoff_nb + pickup_dow:pickup_hour,
                               data = nyc_sql, reportProgress = 0, 
                               rowSelection = (split == "train")))
```

We now point to a new SQL table called `NYCTaxiScore` \(our pointer to it in R will be called `nyc_score`\). 

```{r chap09chunk15}
sqlTable <- "NYCTaxiScore"
nyc_score <- RxSqlServerData(connectionString = sqlConnString,
                             rowsPerRead = sqlRowsPerRead, 
                             table = sqlTable)

rxPredict(linmod, data = nyc_sql, outData = nyc_score,
          predVarNames = "tip_percent_pred_linmod", overwrite = TRUE)
```

Alternatively, we can point to the new data by simply copying and modifying `nyc_sql`. This is probably simpler, since the connection string is still the same as before.

```{r chap09chunk16}
nyc_score <- nyc_sql
nyc_score@table <- "NYCTaxiScore"
```

We now use `rxPredict` to score the `nyc_sql` (the `NYCTaxiSmall` data in SQL) data and output the predictions into `nyc_score` (the `NYCTaxiScore` data in SQL). If any other columns need to also go into the prediction dataset, we can use the `writeModelVars` and `extraVarsToWrite` arguments to `rxPredict` to do that.

```{r chap09chunk17}
rxPredict(linmod, data = nyc_sql, outData = nyc_score,
          predVarNames = "tip_percent_pred_linmod", overwrite = TRUE, 
          extraVarsToWrite = c("pickup_datetime", "dropoff_datetime"))
```

Since building models can be a tedious and sometimes time-consuming process, we usually save models that we build so that we can use them for subsequent scoring \(using `rxPredict`\). Saving models is important because we often score new data on a regular basis using the same model. Saving models in R is easy: we simply use the `save` function, but saving models in SQL using the save function may break the rules about saving external objects on the server. Foretunately, there's a way we can save R objects in R \(not just models but images or any other objects\) by inserting them into the database. In the following example, we begin by creating a table called models with a single column called model whose type is `varbinary(max)`. Moreover, we create a stored procedure in SQL for inserting a model object into this table.

```{SQL, eval=FALSE}
CREATE TABLE models
(model varbinary(max))
GO

CREATE PROCEDURE [dbo].[PersistModel]
@m nvarchar(max)
AS
BEGIN
-- SET NOCOUNT ON added to prevent extra result sets from interfering with SELECT statements.
SET NOCOUNT ON;
insert into models (model) values (convert(varbinary(max),@m,2))
END
```

From R, we can now insert a serialized model object into the above SQL table. To serialize an R object we use the `serialize` function. We then concatenate the serial object into a single \(usually long\) `character` vector \(of length one\) which we can insert into the above table. To run any random SQL query in R we can use the `sqlQuery` function as shown here. In this case, the query simply executes the stored procedure to store the model object in the table we created in the last step.

```{r chap09chunk18}
modelbin <- serialize(linmod, NULL)
modelbinstr <- paste(modelbin, collapse = "")

library(RODBC)
odbcCloseAll()
conn <- odbcDriverConnect(sqlConnString)
q <- paste("EXEC PersistModel @m='", modelbinstr,"'", sep = "")
sqlQuery(conn, q)
```

Once the model object is stored in SQL, to open it in R, we must unserialize the object \(using the `unserialize` R function\). After unserializing it, we obtain the original R object back which can be used to score. So if serializing is the SQL equivalent to the `save` function, unserializing is the equivalent to `load`. We've already seen an example of scoring in dataset using `rxPredict`. Here's an example of doing the same, but by invoking a SQL stored procedure instead of running it directly within R. Before running the stored procedure let's see how we can retrieve the R model object in T-SQL using the `unserialize` function. 

```{sql, eval=FALSE}
-- We can run this to show that we can successfully retrieve the model
DECLARE @lmodel2 varbinary(max) = (SELECT TOP 1 model FROM RDB.dbo.models);
EXEC sp_execute_external_script @language = N'R',
@script = N'
            mod <- unserialize(as.raw(model))
            print(summary(mod))',    
@params = N'@model varbinary(max)',
@model = @lmodel2;  
GO
```

If the model object was successfully retrieved, then it can be passed on to the rxPredict function to score any data with. The stored procedure `PredictTipBatchMode` will score the `NYCTaxiSmall` dataset. **When the model in question involves `factor` columns, it's imperative that the columns are converted to factors, and to make sure that the levels match the levels in the data that was used to create the model in the first place.** In this case, the columns `pickup_dow`, `pickup_hour`, `pickup_nb` and `dropoff_nb` all need to be converted form `character` to `factor` before we run the predictions.

```{sql, eval=FALSE}
-- Create prediction stored procedure
CREATE PROCEDURE [dbo].[PredictTipBatchMode] @inquery nvarchar(max)
AS
BEGIN
DECLARE @lmodel2 varbinary(max) = (SELECT TOP 1 model FROM models);
EXEC sp_execute_external_script @language = N'R',
  @script = N'
              weekday_labels <- c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")
              hour_labels <- c("1AM-5AM", "5AM-9AM", "9AM-12PM", "12PM-4PM", "4PM-6PM", "6PM-10PM", "10PM-1AM")

              library(rgeos)
              library(maptools)

              nyc_shapefile <- readShapePoly("../ZillowNeighborhoods-NY/ZillowNeighborhoods-NY.shp")
              mht_shapefile <- subset(nyc_shapefile, City == "New York" & County == "New York")

              manhattan_nhoods <- subset(nyc_shapefile@data, County == "New York", select = "Name", drop = TRUE)
              manhattan_nhoods <- as.character(manhattan_nhoods)
              bad_nhoods <- c("Brooklyn Heights", "Marble Hill", "Ellis Island", "Liberty Island", "Mill Rock Park", "Governors Island", "Vinegar Hill")
              manhattan_nhoods <- setdiff(manhattan_nhoods, bad_nhoods)

              mod <- unserialize(as.raw(model))

              InputDataSet <- transform(InputDataSet, 
                  pickup_dow = factor(pickup_dow, levels = weekday_labels),
                  pickup_hour = factor(pickup_hour, levels = hour_labels),
                  pickup_nb = factor(pickup_nb, levels = manhattan_nhoods),
                  dropoff_nb = factor(dropoff_nb, levels = manhattan_nhoods))

              OutputDataSet <- rxPredict(modelObject = mod, data = InputDataSet, 
                                         outData = NULL, predVarNames = "Score", 
                                         type = "response", writeModelVars = FALSE, 
                                         overwrite = TRUE)
              str(OutputDataSet)
              print(OutputDataSet)',
  @input_data_1 = @inquery,
  @params = N'@model varbinary(max)',
  @model = @lmodel2
WITH RESULT SETS ((Score float));
END
```

As a SQL stored procedure, it can be invoked by an application that talks to SQL. Here's how we can run the stored procedure from T-SQL:

```{sql, eval=FALSE}
DECLARE @query_string nvarchar(max)
SET @query_string='SELECT top 100 * FROM NYCTaxiSmall'
EXEC [dbo].[PredictTipBatchMode] @inquery = @query_string;
```

Of course the same stored procedure can be invokes directly by R. Here's how:

```{r chap09chunk19}
input <- "N' SELECT top 1000 * FROM NYCTaxiSmall'"
q <- paste("EXEC PredictTipBatchMode @inquery = ", input, sep = "")
scoredData <- sqlQuery(conn, q)
head(scoredData)
```

Let's now look at how visualizations are done in a SQL Server compute context. When creating visualizations in SQL Server we need to consider first if we want to do it in-database or not and second how to store the visualization if it is done in-database. Let's consider several use cases:

If the visualization in question was based on a summary of the data instead of a sample, we could use one of the `RevoScaleR` functions to summarize the data (if the compute context is set to SQL, this happens in-database so there's no need to wrap it in `rxExec`) and then to visualize the summary. Here's an example that involves using `rxQuantile`.

```{r chap09chunk20}
qt <- data.frame(percentile = seq(1, 99, by = 1))
num_vars <- c('fare_amount', 'tip_percent')
qt[ , num_vars] <- lapply(num_vars, function(var) rxQuantile(var, nyc_sql, probs = qt$percentile / 100))
library(ggplot2)
q1 <- ggplot(aes(x = percentile, y = fare_amount), data = qt) + geom_line()
q2 <- ggplot(aes(x = percentile, y = tip_percent), data = qt) + geom_line()

library(gridExtra)
grid.arrange(q1, q2, ncol = 2)
```

Visualizations that are based on the whole data instead of just a summary of the data might take too long to render (and prehaps be useless) when using very large datasets, so instead we rely on sampling to first get the data to a reasonable size. When performing exploratory data analysis (EDA), which usually involves looking at lots of visualizations, sampling can be very effective. We can sample using `rxDataStep` as we learned before.

```{r chap09chunk21}
nyc_sample <- rxDataStep(nyc_sql, rowSelection = (u < .01),
                         transforms = list(u = runif(.rxNumRows)))

library(ggplot2)
ggplot(data = nyc_sample, aes(x = log(trip_distance), y = log(trip_duration))) + 
  geom_point()
```

However this is not very efficient becasue we load the whole data into R before we sample it. To be more efficient, we need to sample the data in SQL and only bring the sample into R. This requires is only a little extra work:

```{r chap09chunk22}
nyc_sample_sql <- nyc_sql
nyc_sample_sql@table <- NULL
nyc_sample_sql@sqlQuery <- 'select * from RDB.dbo.NYCTaxiSmall tablesample (1 percent)'
nyc_sample <- rxImport(nyc_sample_sql)

library(ggplot2)
ggplot(data = nyc_sample, aes(x = log(trip_distance), y = log(trip_duration))) + 
  geom_point()
```

In the above scenario, we sampled the data in SQL and then used the `rxImport` function to bring the sample into R (as a `data.frame`, that is) so that we can plot or otherwise use it. Assuming that the sample is not too big, this usually runs rather quickly. However, using `rxImport` does imply that data is still traveling from the SQL VM to our host machine, so it can be plotted and observed. There is however a more clever way to run the above calculation so that no data (big or small) travel occurs and we only receive the plot object back so we can look at it. The problem is that `ggplot` is not a `RevoScaleR` function and therefore it is not compute-context-aware. This means that if we want to run it in-database, we need to explicitly send it to the SQL VM for execution. To do so we can wrap the above code in a function and pass it to the `rxExec` function for remote execution.

```{r chap09chunk23}
scatterPlot <- function(inDataSource) {
  ds <- rxImport(inDataSource)
  require(ggplot2)
  pl <- ggplot(data = ds, aes(x = log(trip_distance), y = log(trip_duration)))
  pl <- pl + geom_point()
  return(list(myplot = pl))
}

scatterPlot(nyc_sample_sql) # this works, but it's not in-database
```

For this example to run `ggplot2` needs to be installed on the SQL Server R install. We can launch `Rgui.exe` as an administrator, set the `.libPaths()` to be the location where libraries should go, and then install the package. This should usually be done by an admin, not by individual users.

```{r chap09chunk24}
rxSetComputeContext(sqlCC)
myplots <- rxExec(scatterPlot, nyc_sample_sql, timesToRun = 1, packagesToLoad = 'ggplot2')
plot(myplots[[1]][["myplot"]]) # only the plot object is returned to us for display
```

Once the plot object is created in-database, it is up to us to decide what to do with it. In some cases, such as the above example, we wanted to look at the plot in our R IDE. In other cases (especially in production), we are more interested in storing plot objects in-database so that they can later be retrieved by other applications and served on dashboards. We now look at an example of how to do that using SSRS. First let's create some plots and store them in the database.

```{sql, eval=FALSE}
USE RDB;
GO
CREATE TABLE plots(plot varbinary(max));
GO

INSERT INTO plots(plot)
EXEC  sp_execute_external_script
   @language = N'R'
  ,@script = N'
        image_file = tempfile()
        jpeg(filename = image_file, width = 500, height = 500)
        hist(data$fare_amount, col = "light blue")
        dev.off()
        outds <- data.frame(data = readBin(file(image_file, "rb"), what = raw(), n = 1e6))'
  ,@input_data_1 = N'select fare_amount from rdb.dbo.NYCTaxiSmall tablesample (1 percent);'
  ,@input_data_1_name = N'data'
  ,@output_data_1_name = N'outds';
--WITH RESULT SETS ((plot varbinary(max)));
```

Once the plot is stored in the database (in a `varbinary` column), it can be picked up by other applications such as SSRS and served.

